# capture video
import cv2
from mediapipe.tasks.python import vision
import mediapipe as mp
import sys
from PIL import Image

cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_POS_FRAMES, 20)

VisionRunningMode = mp.tasks.vision.RunningMode
model_path = f"models/large.task"

recognizer = vision.GestureRecognizer.create_from_model_path(
    model_path,
)

frames_to_run = 9999999
timestamp = 0

capture = {}

while cap.isOpened() and frames_to_run > 0:
    ret, frame = cap.read()

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

    # convert to 28x28
    # frame = cv2.resize(frame, (28, 28))

    # make grayscale
    # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    cv2.imshow("frame", frame)

    # send frame to recognizer
    image = mp.Image(
        image_format=mp.ImageFormat.SRGB,
        data=frame,
    )

    # Run gesture recognition.
    timestamp += 1
    recognition_result = recognizer.recognize(image)
    # recognition_result = recognizer.recognize

    # Display the most likely gesture.
    top_gesture = recognition_result.gestures

    if len(top_gesture) > 0:
        top_gesture = top_gesture[0][0]
        print(f"Gesture recognized: {top_gesture.category_name} ({top_gesture.score}, frame {timestamp})")

    capture[frames_to_run] = recognition_result   
    frames_to_run -= 1


cap.release()
cv2.destroyAllWindows()
